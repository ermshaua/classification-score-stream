{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis\n",
    "In this notebook, we compare ClaSS with different competitors regarding scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import daproli as dp\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_color_codes()\n",
    "\n",
    "import gc\n",
    "import Orange\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualizer import plot_profile, plot_profile_with_ts\n",
    "from src.utils import load_combined_dataset, load_benchmark_dataset\n",
    "from src.clazz.window_size import suss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose a score for evaluation (F1 or Covering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_score = \"covering_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>window_size</th>\n",
       "      <th>change_points</th>\n",
       "      <th>ts_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adiac</td>\n",
       "      <td>22</td>\n",
       "      <td>[572, 1012, 1232]</td>\n",
       "      <td>1408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ArrowHead</td>\n",
       "      <td>32</td>\n",
       "      <td>[753]</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beef</td>\n",
       "      <td>66</td>\n",
       "      <td>[705]</td>\n",
       "      <td>1410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BeetleFly</td>\n",
       "      <td>34</td>\n",
       "      <td>[1280]</td>\n",
       "      <td>2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BirdChicken</td>\n",
       "      <td>48</td>\n",
       "      <td>[1280]</td>\n",
       "      <td>2560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  window_size      change_points  ts_len\n",
       "0        Adiac           22  [572, 1012, 1232]    1408\n",
       "1    ArrowHead           32              [753]    1506\n",
       "2         Beef           66              [705]    1410\n",
       "3    BeetleFly           34             [1280]    2560\n",
       "4  BirdChicken           48             [1280]    2560"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comb = load_combined_dataset()\n",
    "df_comb['window_size'] = df_comb.time_series.apply(lambda ts: np.int64(suss(ts[:10_000])))\n",
    "df_comb['ts_len'] = df_comb.time_series.apply(len)\n",
    "# takes too much space\n",
    "df_comb.drop(\"time_series\", axis=1, inplace=True)\n",
    "df_comb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the scores from the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▋                                       | 1/8 [00:01<00:07,  1.03s/it]\u001b[A\n",
      " 25%|███████████▎                                 | 2/8 [00:01<00:05,  1.19it/s]\u001b[A\n",
      " 38%|████████████████▉                            | 3/8 [00:44<01:40, 20.01s/it]\u001b[A\n",
      " 50%|██████████████████████▌                      | 4/8 [00:51<00:58, 14.69s/it]\u001b[A\n",
      " 62%|████████████████████████████▏                | 5/8 [03:27<03:17, 65.94s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 6/8 [04:22<02:03, 61.96s/it]\u001b[A\n",
      " 88%|███████████████████████████████████████▍     | 7/8 [06:43<01:27, 87.79s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "competitors = [\"ClaSS\", \"FLOSS\", \"DDM\", \"Window\", \"ADWIN\", \"HDDM\"]\n",
    "datasets = [\"UTSA\", \"TSSB\", \"PAMAP\", \"mHealth\", \"WESAD\", \"MIT-BIH-VE\", \"MIT-BIH-Arr\", \"SleepDB\"]\n",
    "converters = dict([(column, lambda s: np.sum(eval(s))) for column in [\"runtimes\"]])\n",
    "methods, dfs = list(), list()\n",
    "\n",
    "for candidate_name in competitors:\n",
    "    methods.append((candidate_name, f\"{candidate_name}.csv\"))\n",
    "\n",
    "for idx, (name, file_name) in tqdm(enumerate(methods)):\n",
    "    df = []\n",
    "    \n",
    "    for d in tqdm(datasets):\n",
    "        tmp = pd.read_csv(f\"../experiments/competitor_{d}/{file_name}.gz\", usecols=[\"dataset\", eval_score, \"runtimes\"], converters=converters, compression=\"gzip\")\n",
    "        tmp[\"name\"] = d\n",
    "        \n",
    "        df.append(tmp)\n",
    "        gc.collect()\n",
    "        \n",
    "    df = pd.concat(df)    \n",
    "    df.sort_values(by=\"dataset\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    dfs.append((name, df))\n",
    "   \n",
    "df_runtimes = pd.DataFrame()\n",
    "df_throughput = pd.DataFrame()\n",
    "df_scores = pd.DataFrame()\n",
    "\n",
    "for name, df in dfs:\n",
    "    df_runtimes[\"name\"] = df[\"name\"]\n",
    "    df_runtimes[\"dataset\"] = df.dataset\n",
    "    df_runtimes[name] = df.runtimes\n",
    "    \n",
    "    df_throughput[\"name\"] = df[\"name\"]\n",
    "    df_throughput[\"dataset\"] = df.dataset\n",
    "    df_throughput[name] = (df_comb.ts_len / df.runtimes)\n",
    "\n",
    "    df_scores[\"name\"] = df[\"name\"]\n",
    "    df_scores[\"dataset\"] = df.dataset\n",
    "    df_scores[name] = df[eval_score]\n",
    "    \n",
    "idx_sizes = np.argsort(df_comb.ts_len).to_numpy()\n",
    "df_throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the sum/mean/std score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round((df_runtimes.iloc[:,2:].sum(axis=0) / 60) / 60, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(df_throughput.mean(axis=0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_throughput.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the wins per method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = df_runtimes.rank(1, method = 'min', ascending=True)\n",
    "means = np.array(ranks.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mean, name in zip(means, df_runtimes.columns[2:]):\n",
    "    print(f\"{name}:wins={ranks[ranks[name] == 1].shape[0]} rank={np.round(mean, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot and output the ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = Orange.evaluation.scoring.compute_CD(means, df_runtimes.shape[0])\n",
    "Orange.evaluation.scoring.graph_ranks(means, df_runtimes.columns[2:], cd=cd, width=4, reverse=True, textspace=1, filename=f\"../figures/cd_runtime.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute wins/losses against ClaSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = \"ClaSS\"\n",
    "\n",
    "for rival in df_runtimes.columns[2:]:\n",
    "    if rival == selection: continue\n",
    "    \n",
    "    df_pair = df_runtimes[[selection, rival]]\n",
    "    pair_ranks = df_pair.rank(1, method = 'min', ascending=True)\n",
    "    \n",
    "    wins = pair_ranks[(pair_ranks[selection] == 1) & (pair_ranks[rival] == 2)].shape[0]\n",
    "    losses = pair_ranks[(pair_ranks[selection] == 2) & (pair_ranks[rival] == 1)].shape[0]\n",
    "    ties = pair_ranks[(pair_ranks[selection] == 1) & (pair_ranks[rival] == 1)].shape[0]\n",
    "    \n",
    "    assert wins + losses + ties == pair_ranks.shape[0]\n",
    "    \n",
    "    print(f\"{selection} vs {rival}: (wins/ties/losses): {wins}/{ties}/{losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 24\n",
    "_, ax  = plt.subplots(1, figsize=(20,5))\n",
    "\n",
    "for name in df_runtimes.columns[2:]:\n",
    "    ax.plot(df_runtimes[name].to_numpy()[idx_sizes], label=name)\n",
    "    \n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "    \n",
    "ax.tick_params(\n",
    "    axis='x',          \n",
    "    which='both',      \n",
    "    labelbottom=False,\n",
    "    labelleft=False\n",
    ")\n",
    "ax.legend(loc=2, prop={'size': fontsize})\n",
    "ax.set_xlabel(\"All TS data sets (ordered by length)\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"runtime in seconds\", fontsize=fontsize)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "names = df_runtimes.name.to_numpy().tolist()      \n",
    "names = np.array(names)[idx_sizes]\n",
    "\n",
    "plt.savefig(f\"../figures/runtime.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 24\n",
    "\n",
    "ax = df_runtimes.groupby(\"name\").sum().T[['UTSA','TSSB','PAMAP','mHealth','WESAD','SleepDB','MIT-BIH-Arr','MIT-BIH-VE']].T.apply(lambda x: x / 60).plot(kind=\"bar\", figsize=(8,5), rot=45)\n",
    "    \n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.legend(prop={'size': fontsize})\n",
    "ax.set_xlabel(\"\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"runtime in minutes\", fontsize=fontsize)\n",
    "\n",
    "\n",
    "plt.savefig(f\"../figures/bar_runtime.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "df_runtimes.groupby(\"name\").sum() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 24\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "for competitor in df_runtimes.columns[2:]:\n",
    "    ax.scatter(df_runtimes[competitor].mean(), df_scores[competitor].mean(), label=competitor)\n",
    "\n",
    "ax.set_xlabel(\"runtime in seconds\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Covering\", fontsize=fontsize)\n",
    "\n",
    "ax.legend(prop={'size': fontsize})\n",
    "#ax.set_yscale('log')\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "\n",
    "plt.savefig(f\"../figures/runtime_covering.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "lr = LinearRegression().fit(df_comb.ts_len.to_numpy().reshape(-1,1), df_runtimes.ClaSS.to_numpy().reshape(-1,1))\n",
    "y_min, y_max = lr.predict(np.array([df_comb.ts_len.min(), df_comb.ts_len.max()]).reshape(-1,1)).flatten()\n",
    "\n",
    "# mean deviation predicted / actual ts length\n",
    "np.mean(np.abs(1 - lr.predict(df_comb.ts_len.to_numpy().reshape(-1,1)).flatten() / df_runtimes.ClaSS.to_numpy()))\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Mean absolute error: {np.round(mean_absolute_error(df_runtimes.ClaSS.to_numpy(), lr.predict(df_comb.ts_len.to_numpy().reshape(-1,1)).flatten()), 3)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 24\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(10,3))\n",
    "ax.scatter(df_comb.ts_len, df_runtimes.ClaSS / 60)\n",
    "\n",
    "ax.plot([df_comb.ts_len.min(), df_comb.ts_len.max()], [y_min / 60, y_max / 60], c=\"C1\", label=\"Linear Regression\")\n",
    "\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel(\"time series length\", fontsize=24)\n",
    "ax.set_ylabel(\"runtime in minutes\", fontsize=24)\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(20)\n",
    "\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "    \n",
    "ax.legend(loc=2, prop={'size': fontsize-4})\n",
    "\n",
    "plt.savefig(f\"../figures/runtime_length.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 24\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(10,5))\n",
    "ax.scatter(df_comb[\"change_points\"].apply(len), df_runtimes.ClaSS)\n",
    "ax.set_xlabel(\"amount of change points\", fontsize=24)\n",
    "ax.set_ylabel(\"runtime in seconds\", fontsize=24)\n",
    "\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "\n",
    "plt.savefig(f\"../figures/runtime_cps.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 24\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(10,5))\n",
    "ax.scatter(df_comb.window_size, df_runtimes.ClaSS)\n",
    "ax.set_xlabel(\"window size\", fontsize=20)\n",
    "ax.set_ylabel(\"runtime in seconds\", fontsize=20)\n",
    "\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "\n",
    "plt.savefig(f\"../figures/runtime_window.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 24\n",
    "_, ax = plt.subplots(figsize=(4,5))\n",
    "\n",
    "df_throughput.boxplot(ax=ax, rot=90)\n",
    "ax.set_ylabel(\"Data points per second\", fontsize=fontsize)\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(fontsize)\n",
    "    \n",
    "ax.set_xticklabels(df_scores.columns[2:], rotation=45, ha=\"right\")\n",
    "plt.savefig(f\"../figures/bp_throughput.pdf\", bbox_inches=\"tight\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_throughput.ClaSS.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
